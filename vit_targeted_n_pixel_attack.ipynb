{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3852d7fb",
   "metadata": {},
   "source": [
    "\n",
    "# Targeted N-Pixel Attack on Vision Transformer (ViT)\n",
    "\n",
    "This notebook implements a **black-box adversarial attack** on a **pretrained Vision Transformer (ViT)**.\n",
    "The attack generalizes the *One Pixel Attack* to an **N-pixel setting** and supports **targeted attacks**.\n",
    "\n",
    "Key properties:\n",
    "- Model: Pretrained ViT (ImageNet)\n",
    "- Attack type: Black-box (no gradients)\n",
    "- Optimization: Differential Evolution (DE)\n",
    "- Perturbation budget: N pixels (user-defined)\n",
    "- Objective: Targeted misclassification\n",
    "\n",
    "This notebook is designed for **research, robustness evaluation, and educational purposes**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e941f1",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40d3edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import models\n",
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6174d9c1",
   "metadata": {},
   "source": [
    "## 2. Load Pretrained Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c6cb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load a pretrained ViT-B/16 model\n",
    "model = models.vit_b_16(weights=models.ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# ImageNet normalization\n",
    "weights = models.ViT_B_16_Weights.IMAGENET1K_V1\n",
    "preprocess = weights.transforms()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110ba8a7",
   "metadata": {},
   "source": [
    "## 3. Load and Preprocess an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6633c1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Load a sample image (replace with your own image if desired)\n",
    "img_path = \"sample.jpg\"  # user-provided image\n",
    "img_pil = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "img = preprocess(img_pil).unsqueeze(0).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5f33cf",
   "metadata": {},
   "source": [
    "## 4. Prediction Utility (Black-Box Access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7924e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(image_tensor):\n",
    "    with torch.no_grad():\n",
    "        logits = model(image_tensor)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "    return probs.cpu().numpy()[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596d463c",
   "metadata": {},
   "source": [
    "## 5. N-Pixel Perturbation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecba78d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_n_pixel_attack(image, params, n_pixels):\n",
    "    perturbed = image.clone()\n",
    "\n",
    "    for i in range(n_pixels):\n",
    "        base = i * 5\n",
    "        x = int(params[base + 0])\n",
    "        y = int(params[base + 1])\n",
    "        r = params[base + 2]\n",
    "        g = params[base + 3]\n",
    "        b = params[base + 4]\n",
    "\n",
    "        perturbed[0, 0, y, x] = r\n",
    "        perturbed[0, 1, y, x] = g\n",
    "        perturbed[0, 2, y, x] = b\n",
    "\n",
    "    return torch.clamp(perturbed, 0, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f90d88",
   "metadata": {},
   "source": [
    "## 6. Targeted Attack Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b857dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def targeted_objective(params, image, target_class, n_pixels):\n",
    "    perturbed = apply_n_pixel_attack(image, params, n_pixels)\n",
    "    probs = predict(perturbed)\n",
    "    # Minimize negative confidence of the target class (maximize target probability)\n",
    "    return -probs[target_class]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9624683b",
   "metadata": {},
   "source": [
    "## 7. Run Targeted N-Pixel Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c232a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# User-defined parameters\n",
    "N_PIXELS = 3           # Number of pixels to modify\n",
    "TARGET_CLASS = 859     # Example target class (e.g., toaster in ImageNet)\n",
    "\n",
    "orig_probs = predict(img)\n",
    "orig_pred = np.argmax(orig_probs)\n",
    "\n",
    "print(\"Original prediction:\", orig_pred)\n",
    "print(\"Target class:\", TARGET_CLASS)\n",
    "\n",
    "# Image size after preprocessing (ViT default: 224x224)\n",
    "H, W = img.shape[-2], img.shape[-1]\n",
    "\n",
    "bounds = []\n",
    "for _ in range(N_PIXELS):\n",
    "    bounds.extend([\n",
    "        (0, W - 1),  # x\n",
    "        (0, H - 1),  # y\n",
    "        (0, 1),      # R\n",
    "        (0, 1),      # G\n",
    "        (0, 1)       # B\n",
    "    ])\n",
    "\n",
    "result = differential_evolution(\n",
    "    targeted_objective,\n",
    "    bounds,\n",
    "    args=(img, TARGET_CLASS, N_PIXELS),\n",
    "    maxiter=100,\n",
    "    popsize=15,\n",
    "    disp=True\n",
    ")\n",
    "\n",
    "best_params = result.x\n",
    "adv_img = apply_n_pixel_attack(img, best_params, N_PIXELS)\n",
    "\n",
    "adv_probs = predict(adv_img)\n",
    "adv_pred = np.argmax(adv_probs)\n",
    "\n",
    "print(\"Adversarial prediction:\", adv_pred)\n",
    "print(\"Target confidence:\", adv_probs[TARGET_CLASS])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ae082a",
   "metadata": {},
   "source": [
    "## 8. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fe1c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_image(tensor, title):\n",
    "    img = tensor.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "    plt.imshow(img)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "show_image(img, f\"Original (class {orig_pred})\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "show_image(adv_img, f\"Adversarial (class {adv_pred})\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da6aa98",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Summary and Notes\n",
    "\n",
    "- This notebook demonstrates a **targeted N-pixel black-box attack** on a pretrained ViT.\n",
    "- The attack modifies a very small number of pixels while achieving targeted misclassification.\n",
    "- Increasing N improves attack success but increases perceptibility and search complexity.\n",
    "\n",
    "Possible extensions:\n",
    "- Compare ViT vs CNN robustness\n",
    "- Add query budget constraints\n",
    "- Replace Differential Evolution with CMA-ES or NES\n",
    "- Perform batch-level success rate evaluation\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
